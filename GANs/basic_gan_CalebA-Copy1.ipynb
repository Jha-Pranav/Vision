{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da03237b-025f-4e84-b919-2c75e5b792fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc84e7f1-3626-4669-9b73-264805af4bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "DATASET_PATH = os.environ.get('DATASET_PATH',\"./data\")\n",
    "DATASET_PATH += \"/celeba/img_align_celeba/\"\n",
    "BATCH_SIZE = 728 if torch.cuda.is_available() else 64\n",
    "NUM_WORKERS = int(os.cpu_count() / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f62e9980-df54-484f-af80-bada2763a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "102377e1-930c-4c8b-bd77-03b949eac4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/pranav-pc/projects/DATA//celeba/img_align_celeba/'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97004cdc-b13e-4560-afe3-dfd87cdfeefd",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset not found or corrupted. You can use download=True to download it",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCelebA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATASET_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.env/lib/python3.12/site-packages/torchvision/datasets/celeba.py:88\u001b[0m, in \u001b[0;36mCelebA.__init__\u001b[0;34m(self, root, split, target_type, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload()\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     90\u001b[0m split_map \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     95\u001b[0m }\n\u001b[1;32m     96\u001b[0m split_ \u001b[38;5;241m=\u001b[39m split_map[verify_str_arg(split\u001b[38;5;241m.\u001b[39mlower(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m))]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dataset not found or corrupted. You can use download=True to download it"
     ]
    }
   ],
   "source": [
    "datasets.CelebA(root=DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "772bd031-7ca3-4a2a-9763-be545348e283",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Celeba(pl.LightningDataModule):\n",
    "    def __init__(self, root, imgsz, batchsz, no_workers):\n",
    "        super(Celeba, self).__init__()\n",
    "        self.root = root\n",
    "        self.imgsz = imgsz\n",
    "        self.batchsz = batchsz\n",
    "        self.num_workers = no_workers\n",
    "\n",
    "        self.transform = transforms.Compose(\n",
    "            [transforms.Resize(self.imgsz),\n",
    "             transforms.CenterCrop(self.imgsz),\n",
    "             # transforms.Grayscale(num_output_channels=1),\n",
    "             #transforms.AutoAugment(), #imagenet\n",
    "               \n",
    "             transforms.ToTensor(),\n",
    "            transforms.Normalize(mean,std)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def setup(self, stage):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train_dataset = datasets.ImageFolder(\n",
    "                root=self.root,transform=self.transform\n",
    "            )\n",
    "            # self.val_dataset = datasets.ImageFolder(\n",
    "            #     root=os.path.join(self.root, \"validation\"),transform=self.transform\n",
    "            # )\n",
    "\n",
    "        # if stage == \"test\" or stage is None:\n",
    "        #     self.test_dataset = datasets.ImageFolder(\n",
    "        #         root=os.path.join(self.root, \"test\"),transform=self.transform\n",
    "        #     )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batchsz,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    # def val_dataloader(self):\n",
    "    #     return DataLoader(\n",
    "    #         self.val_dataset,\n",
    "    #         batch_size=self.batchsz,\n",
    "    #         shuffle=False,\n",
    "    #         num_workers=self.num_workers,\n",
    "    #     )\n",
    "\n",
    "    # def test_dataloader(self):\n",
    "    #     return DataLoader(\n",
    "    #         self.test_dataset,\n",
    "    #         batch_size=self.batchsz,\n",
    "    #         shuffle=False,\n",
    "    #         num_workers=self.num_workers,\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e4b36fe-0270-4d72-b4c2-c7c2d090ed64",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 64\n",
    "ds = Celeba(DATASET_PATH,IMAGE_SIZE,BATCH_SIZE,NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc1f19dc-b5f9-4ce7-887e-c2f8fc7e79da",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find any class folder in /home/pranav-pc/projects/DATA//celeba/img_align_celeba.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Sanity Check\u001b[39;00m\n\u001b[1;32m      4\u001b[0m ds\u001b[38;5;241m.\u001b[39mprepare_data()\n\u001b[0;32m----> 5\u001b[0m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mtrain_dataloader()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Create a dictionary to store one image per class\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 22\u001b[0m, in \u001b[0;36mCeleba.setup\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup\u001b[39m(\u001b[38;5;28mself\u001b[39m, stage):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stage \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m stage \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.env/lib/python3.12/site-packages/torchvision/datasets/folder.py:328\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    321\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    327\u001b[0m ):\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[0;32m~/.env/lib/python3.12/site-packages/torchvision/datasets/folder.py:149\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    140\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 149\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[1;32m    152\u001b[0m         class_to_idx\u001b[38;5;241m=\u001b[39mclass_to_idx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m         allow_empty\u001b[38;5;241m=\u001b[39mallow_empty,\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[0;32m~/.env/lib/python3.12/site-packages/torchvision/datasets/folder.py:234\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.env/lib/python3.12/site-packages/torchvision/datasets/folder.py:43\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     41\u001b[0m classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mscandir(directory) \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m class_to_idx \u001b[38;5;241m=\u001b[39m {cls_name: i \u001b[38;5;28;01mfor\u001b[39;00m i, cls_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(classes)}\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m classes, class_to_idx\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in /home/pranav-pc/projects/DATA//celeba/img_align_celeba."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sanity Check\n",
    "ds.prepare_data()\n",
    "ds.setup('fit')\n",
    "dataloader = ds.train_dataloader()\n",
    "\n",
    "\n",
    "# Create a dictionary to store one image per class\n",
    "class_images = {}\n",
    "\n",
    "# Iterate over the dataset and store one image per class\n",
    "for image, label in ds.train_dataset:\n",
    "\n",
    "    class_images[label] = image\n",
    "    if len(class_images) == 10:  # Stop after storing one image per class\n",
    "        break\n",
    "\n",
    "# Display the images from each class\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "for i, (label, image) in enumerate(class_images.items()):\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    ax.imshow(image.permute(1,2,0))\n",
    "    ax.set_title(f\"Class: {label}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "132c55eb-9c7a-4071-9822-1fdbf74bb2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, img_shape):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def _block(in_feature, out_feature, normalize=True):\n",
    "            layers = [nn.Linear(in_feature, out_feature)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feature,0.8))\n",
    "            layers.append(nn.LeakyReLU(0.01))\n",
    "            return layers\n",
    "\n",
    "        self.g_model = nn.Sequential(\n",
    "            *_block(latent_dim, 128, normalize=False),\n",
    "            *_block(128, 256),\n",
    "            *_block(256, 512),\n",
    "            *_block(512, 1024),\n",
    "            nn.Linear(1024, img_shape),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.g_model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20575575-980a-4ef5-a92c-f01d8c8f046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,in_feature):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.d_model = nn.Sequential(\n",
    "                                 nn.Linear(in_feature,512),\n",
    "                                 nn.LeakyReLU(negative_slope=0.2),\n",
    "                                 nn.Linear(512,256),\n",
    "                                 nn.LeakyReLU(0.2),\n",
    "                                 nn.Linear(256,1),\n",
    "                                 nn.Sigmoid())\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.d_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70a1e4e2-b666-4fb0-8225-50408da6a354",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(pl.LightningModule):\n",
    "    def __init__(self,latent_dim,img_shape,lr,b1,b2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.generator = Generator(latent_dim,img_shape)\n",
    "        self.discriminator = Discriminator(img_shape)\n",
    "\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "    def forward(self,z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def adversarial_loss(self,y_hat,y):\n",
    "        return nn.functional.binary_cross_entropy(y_hat,y)\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        imgs, _ = batch \n",
    "\n",
    "        # sample noise \n",
    "        z = torch.randn(imgs.shape[0],self.hparams.latent_dim).type_as(imgs)\n",
    "\n",
    "\n",
    "        # ground truth \n",
    "        label_one = torch.ones(imgs.shape[0],1).type_as(imgs)\n",
    "        label_zero = torch.zeros(imgs.shape[0],1).type_as(imgs)\n",
    "\n",
    "        # Get optimizers\n",
    "        g_opt, d_opt = self.optimizers()\n",
    "\n",
    "        # Train Generator\n",
    "        self.toggle_optimizer(g_opt)\n",
    "\n",
    "        self.generated_img = self(z)\n",
    "\n",
    "        #log \n",
    "        if not batch_idx % 100:\n",
    "            sample_imgs = self.generated_img[:6]\n",
    "            grid = torchvision.utils.make_grid(sample_imgs.view(-1,3,IMAGE_SIZE,IMAGE_SIZE))\n",
    "            self.logger.experiment.add_image('generated_images',grid,self.current_epoch)\n",
    "        \n",
    "        g_loss = self.adversarial_loss(self.discriminator(self.generated_img),label_one)\n",
    "        self.log('g_loss',g_loss,prog_bar=True)\n",
    "\n",
    "        self.manual_backward(g_loss)\n",
    "        g_opt.step()\n",
    "        g_opt.zero_grad()\n",
    "        self.untoggle_optimizer(g_opt)\n",
    "\n",
    "\n",
    "        # Train discriminator\n",
    "        self.toggle_optimizer(d_opt)\n",
    "        # how well it can be label as real\n",
    "        real_loss = self.adversarial_loss(self.discriminator(imgs.view(imgs.shape[0],-1)),label_one)\n",
    "        # how well it can be label as fake\n",
    "        fake_loss = self.adversarial_loss(self.discriminator(self.generated_img.detach()),label_zero)\n",
    "\n",
    "        d_loss = (real_loss + fake_loss)/2\n",
    "        self.log('d_loss',d_loss,prog_bar=True)\n",
    "\n",
    "        self.manual_backward(d_loss)\n",
    "        d_opt.step()\n",
    "        d_opt.zero_grad()\n",
    "        self.untoggle_optimizer(d_loss)\n",
    "\n",
    "\n",
    "    def validation_step(self,batch):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        g_opt = torch.optim.AdamW(self.generator.parameters(),lr=self.hparams.lr, betas=[self.hparams.b1,self.hparams.b2])\n",
    "        d_opt = torch.optim.AdamW(self.discriminator.parameters(),lr=self.hparams.lr, betas=[self.hparams.b1,self.hparams.b2])            \n",
    "            \n",
    "        return [g_opt,d_opt], []\n",
    "\n",
    "    def on_validation_batch_end(self):\n",
    "        validation_noise = torch.randn(8,self.hparams.latent_dim).type_as(self.generator.model[0].weight)\n",
    "                    \n",
    "        #log \n",
    "        sample_imgs = self(validation_noise)\n",
    "        grid = torchvision.utils.make_grid(sample_imgs.view(-1,3,IMAGE_SIZE,IMAGE_SIZE))\n",
    "        self.logger.experiment.add_image('Validation_z',grid,self.current_epoch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1ca491a-ebfc-4700-b6b4-213654e44dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim, img_shape, lr, b1, b2 = 100, 3*IMAGE_SIZE*IMAGE_SIZE, 2e-4, 0.5, 0.999\n",
    "model = GAN(latent_dim, img_shape, lr, b1,b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46d3619c-3d3e-4870-8722-7e1831686d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd1bd24-2b35-4912-82c8-567eb4893f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/pranav-pc/.env/lib/python3.12/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadb406acc7e4d87a7a0467828862fdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = pl.Trainer(min_epochs=1,\n",
    "                        max_epochs=2000 ,\n",
    "                        # precision='16-mixed',\n",
    "                        enable_model_summary=False,\n",
    "                        # callbacks=[pl.callbacks.EarlyStopping('d_loss',min_delta=0.0,patience=5)],\n",
    "                        enable_checkpointing  = True,\n",
    "                        log_every_n_steps=10\n",
    "                    )\n",
    "trainer.fit(model,ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3626b950-9399-4172-9422-bd92b191ce00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision-lab",
   "language": "python",
   "name": "vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
