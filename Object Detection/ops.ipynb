{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d265292b-1470-4c5f-a3e0-dacde15ecaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe73b3dc-0df0-4e88-b237-ca5dc15339c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4706, 0.0000],\n",
      "        [0.3245, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.ops as ops\n",
    "\n",
    "# Define bounding boxes\n",
    "boxes1 = torch.tensor([\n",
    "    [10, 10, 20, 20],\n",
    "    [15, 15, 25, 25]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "boxes2 = torch.tensor([\n",
    "    [12, 12, 22, 22],\n",
    "    [30, 30, 40, 40]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Compute IoU\n",
    "iou_matrix = ops.box_iou(boxes1, boxes2)\n",
    "\n",
    "print(iou_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c0d9430-3906-45a8-b6f5-599c12a03b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Non MaX SUPRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "114a86b3-b8c7-4d74-9e7b-d3be71808a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept boxes:\n",
      "tensor([[ 50.,  50., 200., 200.],\n",
      "        [200., 200., 300., 300.]])\n",
      "Kept scores:\n",
      "tensor([0.9000, 0.8500])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.ops as ops\n",
    "\n",
    "# Define bounding boxes and scores\n",
    "boxes = torch.tensor([\n",
    "    [50, 50, 200, 200],\n",
    "    [55, 55, 210, 210],\n",
    "    [200, 200, 300, 300],\n",
    "    [220, 220, 310, 310]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "scores = torch.tensor([0.9, 0.75, 0.85, 0.8], dtype=torch.float32)\n",
    "\n",
    "# Set IoU threshold\n",
    "iou_threshold = 0.5\n",
    "\n",
    "# Perform NMS\n",
    "keep = ops.nms(boxes, scores, iou_threshold)\n",
    "\n",
    "# Filter the bounding boxes and scores\n",
    "kept_boxes = boxes[keep]\n",
    "kept_scores = scores[keep]\n",
    "\n",
    "print(\"Kept boxes:\")\n",
    "print(kept_boxes)\n",
    "print(\"Kept scores:\")\n",
    "print(kept_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff16e260-438b-45d1-a200-9ebaa24f14a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MeanAveragePrecision.update() takes 3 positional arguments but 6 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m metric \u001b[38;5;241m=\u001b[39m torchmetrics\u001b[38;5;241m.\u001b[39mdetection\u001b[38;5;241m.\u001b[39mMeanAveragePrecision()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Update the metric with predictions and ground truth\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_boxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_boxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Compute the mean average precision\u001b[39;00m\n\u001b[1;32m     28\u001b[0m mAP \u001b[38;5;241m=\u001b[39m metric\u001b[38;5;241m.\u001b[39mcompute()\n",
      "File \u001b[0;32m~/.env/lib/python3.12/site-packages/torchmetrics/metric.py:483\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad):\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 483\u001b[0m         \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "\u001b[0;31mTypeError\u001b[0m: MeanAveragePrecision.update() takes 3 positional arguments but 6 were given"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "# Define ground truth and predictions\n",
    "gt_boxes = torch.tensor([\n",
    "    [50, 50, 200, 200],\n",
    "    [30, 30, 100, 100]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "gt_labels = torch.tensor([0, 1], dtype=torch.int64)\n",
    "\n",
    "pred_boxes = torch.tensor([\n",
    "    [48, 48, 198, 198],\n",
    "    [70, 70, 130, 130],\n",
    "    [250, 250, 300, 300]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "pred_scores = torch.tensor([0.9, 0.75, 0.6], dtype=torch.float32)\n",
    "pred_labels = torch.tensor([0, 1, 2], dtype=torch.int64)\n",
    "\n",
    "# Initialize the metric\n",
    "metric = torchmetrics.detection.MeanAveragePrecision()\n",
    "\n",
    "# Update the metric with predictions and ground truth\n",
    "metric.update(pred_boxes, pred_scores, pred_labels, gt_boxes, gt_labels)\n",
    "\n",
    "# Compute the mean average precision\n",
    "mAP = metric.compute()\n",
    "print(\"Mean Average Precision (mAP):\", mAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316eb444-0b07-43cd-be73-b0f4516b7fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision-lab",
   "language": "python",
   "name": "vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
